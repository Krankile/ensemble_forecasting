{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krankile/ensemble_forecasting/blob/main/notebooks/autoencoder/1_fit_lstm_ae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucj1BmWBwFT5"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WN7O_JPzffp"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rflHmkkgMhbx"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPWztpjG_UUc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/Krankile/ensemble_forecasting.git\n",
        "!mv ensemble_forecasting ef"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!cd ef && git pull"
      ],
      "metadata": {
        "id": "fVsa0PflCLpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb as wb\n",
        "wb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrXbxZSrCqoW",
        "outputId": "1af7bd0d-4fdb-4cc0-d662-9620fee6df30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkrankile\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NOjedVwhAX9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e574da-89a7-4154-c58a-dc256c61e3a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import os\n",
        "import copy\n",
        "import random\n",
        "from datetime import datetime as dt\n",
        "import psutil\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import json\n",
        "from importlib import reload\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch import nn\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "from torch.nn.utils.rnn import pack_sequence, pack_padded_sequence, pad_packed_sequence, pad_sequence, PackedSequence\n",
        "\n",
        "from ef.models import lstm_autoencoders\n",
        "from ef.plotting.ae_plot import plot_examples\n",
        "from ef.utils import normalizers, schedulers, optimizers\n",
        "from ef.data import autoencoder_loaders\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (16, 8)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVeue_WairST"
      },
      "source": [
        "## Run training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xj1P37cLFxi"
      },
      "source": [
        "### Setup and start training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqAon4--m1EM"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, conf):\n",
        "\n",
        "    optimizer = optimizers[conf.optimizer](model.parameters(), lr=conf.learning_rate, weight_decay=conf.weight_decay)\n",
        "    criterion = nn.L1Loss(reduction=\"mean\").to(device)\n",
        "    scheduler = schedulers[conf.scheduler[\"name\"] if conf.scheduler else None](optimizer, **conf.scheduler[\"kwargs\"] if conf.scheduler else {})\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    best_loss = float(\"inf\")\n",
        "    b_size = conf.batch_size\n",
        "    step = 0\n",
        "    example_data, example_lens, _ = next(iter(val_loader))\n",
        "    example_packed = pack_padded_sequence(example_data, example_lens, batch_first=True, enforce_sorted=False)\n",
        "    history = dict(train=[], val=[])\n",
        "\n",
        "    it = tqdm(range(1, conf.epochs + 1))\n",
        "    for epoch in it:\n",
        "        \n",
        "        # Training part of epoch\n",
        "        model = model.train()\n",
        "        train_losses = []\n",
        "        for seq_true, lens, _ in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            packed_true = pack_padded_sequence(seq_true, lens, batch_first=True, enforce_sorted=False).to(device)\n",
        "            seq_true = seq_true.to(device)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                seq_pred = model(packed_true, lens)\n",
        "                loss = criterion(seq_pred.data, packed_true.data)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            step += 1\n",
        "\n",
        "        # Validation part of epoch\n",
        "        val_losses = []\n",
        "        model = model.eval()\n",
        "        with torch.no_grad():\n",
        "            for seq_true, lens, _ in val_loader:\n",
        "                seq_true = seq_true.to(device)\n",
        "                packed_true = pack_padded_sequence(seq_true, lens, batch_first=True, enforce_sorted=False).to(device)\n",
        "                seq_pred = model(packed_true, lens)\n",
        "                \n",
        "                loss = criterion(seq_pred.data, packed_true.data)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        val_loss = np.mean(val_losses)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        history[\"train\"].append(train_loss)\n",
        "        history[\"val\"].append(val_loss)\n",
        "\n",
        "        wb.log({\"train_loss\": train_loss, \"val_loss\": val_loss, \"epoch\": epoch, \"examples\": step*b_size, \"lr\": optimizer.param_groups[0][\"lr\"]}, step=step)\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            filepath = \"model.torch\"\n",
        "            torch.save(best_model_wts, filepath)\n",
        "            wb.save(filepath)\n",
        "\n",
        "            figurepath = \"best_val_plot.png\"\n",
        "            plot_examples(\n",
        "                figurepath,\n",
        "                example_data,\n",
        "                example_packed.to(device),\n",
        "                model,\n",
        "                lens=example_lens,\n",
        "                conf=conf,\n",
        "            )\n",
        "            wb.log({\"best_val_expl\": wb.Image(figurepath), \"epoch\": epoch}, step=step)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            figurepath = \"periodic_val_plot.png\"\n",
        "            plot_examples(\n",
        "                figurepath,\n",
        "                example_data,\n",
        "                example_packed.to(device),\n",
        "                model,\n",
        "                lens=example_lens,\n",
        "                conf=conf,\n",
        "            )\n",
        "            wb.log({\"periodic_val_expl\": wb.Image(figurepath), \"epoch\": epoch}, step=step)\n",
        "\n",
        "        it.set_postfix(\n",
        "            train_loss=train_loss,\n",
        "            val_loss=val_loss,\n",
        "            lr=f\"{optimizer.param_groups[0]['lr']:.2e}\",\n",
        "        )\n",
        "\n",
        "        # Code for early stopping\n",
        "        if conf.get(\"early_stop\") is not None and early_stop(history, ma1=5, ma2=20, em=1.05):\n",
        "            wb.log({\"early_stop\": True, \"epoch\": epoch}, step=step)\n",
        "            break\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model.eval(), filepath"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def df_from_art(run, artname, *, root=\"krankile/data-processing/\"):\n",
        "        art = run.use_artifact(root + artname); art.download()\n",
        "        df = pd.read_feather(art.file()).set_index(\"m4id\")\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "6gV6f5CRHhSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYKa_AzAyWxE"
      },
      "outputs": [],
      "source": [
        "def train(config=None, project=None, entity=None, enablewb=True):\n",
        "    mode = \"online\" if enablewb else \"disabled\"\n",
        "    with wb.init(config=config, project=project, entity=entity, job_type=\"training\", mode=mode) as run:\n",
        "        conf = run.config\n",
        "\n",
        "        series, info, split = map(partial(df_from_art, run), (\"series_traval:latest\", \"info_traval:latest\", \"traval_split_80_20:latest\"))\n",
        "        traidx, validx = split[split.val == False].index, split[split.val == True].index\n",
        "        tra_data = dict(\n",
        "            series=series.loc[traidx],\n",
        "            info=info.loc[traidx],\n",
        "        )\n",
        "        val_data = dict(\n",
        "            series=series.loc[validx],\n",
        "            info=info.loc[validx],\n",
        "        )\n",
        "\n",
        "        (train_loader,\n",
        "         val_loader,\n",
        "         seq_len,\n",
        "         n_features) = autoencoder_loaders(run, tra_data, val_data, cpus=None)\n",
        "\n",
        "        model = lstm_autoencoders[conf.architecture](\n",
        "            seq_len=seq_len,\n",
        "            n_features=n_features,\n",
        "            embedding_dim=conf.embedding_dim,\n",
        "            hidden_dim=conf.hidden_dim,\n",
        "            dropout=conf.dropout,\n",
        "            num_layers=conf.num_layers,\n",
        "        )\n",
        "\n",
        "        print(f\"Moving model {conf.architecture} to device: {device}\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        model, savepath = train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            conf=conf,\n",
        "        )\n",
        "\n",
        "        artifact = wb.Artifact(conf.architecture, type='lstm-ae-model', metadata={\"config\": json.dumps(dict(conf))})\n",
        "        # Add a file to the artifact's contents\n",
        "        artifact.add_file(savepath)\n",
        "        # Save the artifact version to W&B and mark it as the output of this run\n",
        "        run.log_artifact(artifact)\n",
        "    return model\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etegyX2aucFF"
      },
      "source": [
        "### Standalone training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJm3PHSqCN2X"
      },
      "source": [
        "#### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYlijeB9ug1d"
      },
      "outputs": [],
      "source": [
        "config = dict(\n",
        "    epochs=500,\n",
        "    maxlen=250,\n",
        "    embedding_dim=32,\n",
        "    hidden_dim=128,\n",
        "    learning_rate=0.002,\n",
        "    architecture=\"RecurrentAutoencoderV4\",\n",
        "    num_layers=2,\n",
        "    batch_size=256*2,\n",
        "    optimizer=\"adamw\",\n",
        "    dropout=0.2,\n",
        "    normalize_data=\"normal\",\n",
        "    weight_decay=0.005,\n",
        "    scheduler=None  # {\"name\": \"MultiStepLR\", \"kwargs\": {\"milestones\": [100, 200, 400, 800], \"gamma\": 0.5}},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYFXrRwzCPtI"
      },
      "source": [
        "#### Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iZHRgzWu8yF"
      },
      "outputs": [],
      "source": [
        "enablewb = True\n",
        "sweepid = None\n",
        "\n",
        "if sweepid is not None:\n",
        "    count = 100 # number of runs to execute\n",
        "    wb.agent(sweepid, function=partial(train, config=config), count=count)\n",
        "else:\n",
        "    model = train(config=config, project=\"lstm-ae-tmp\", entity=\"krankile\", enablewb=enablewb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mz73HnDJY_C"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "1_fit_lstm_ae.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}