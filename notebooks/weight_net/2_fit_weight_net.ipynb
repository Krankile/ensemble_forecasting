{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krankile/ensemble_forecasting/blob/main/notebooks/weight_net/2_fit_weight_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad2rmI-1IBOy"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Data set classes expect data to be normalized"
      ],
      "metadata": {
        "id": "Aydik2Y7JSCm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3adfcnQXtr4x"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rbUYg-ygNho7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs7aB3tB1uMl"
      },
      "source": [
        "Go here to find wandb API key:\n",
        "\n",
        "[https://wandb.ai/settings](https://wandb.ai/settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbiRr31H-t9b",
        "outputId": "036ea416-d3b4-4eda-eb6e-893964a0397f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkrankile\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import wandb as wb\n",
        "wb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L6jIfYgytp_w"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/Krankile/ensemble_forecasting.git\n",
        "!mv ensemble_forecasting ef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sjj7i6KDsrRa"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!cd ef && git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-Oo7lgF2HeDZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "from multiprocessing import cpu_count\n",
        "from pathlib import Path\n",
        "from collections import namedtuple\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from ef.models import weightnets\n",
        "from ef.utils import loss_functions, activations, optimizers, schedulers, scalers\n",
        "\n",
        "from ef.data import ensemble_loaders, ensemble_loaders_kfold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbGDaQzgSr7y",
        "outputId": "1e868ff8-9fdc-428f-b002-36c8f75cab03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def artifact_to_path(run, art_name, *, root=\"krankile/data-processing/\"):\n",
        "    art = run.use_artifact(root + art_name); art.download()\n",
        "    return art.file()"
      ],
      "metadata": {
        "id": "uUQ_KlYfK3KN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debug area"
      ],
      "metadata": {
        "id": "KMfvZQ5aMnPu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhZaI8E_IGcJ"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJJhNFTgq2YF"
      },
      "source": [
        "## Normal train-val split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HafRiXtVZ80M"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, num_examples, conf):    \n",
        "    batch_size = conf.batch_size\n",
        "\n",
        "    optimizer = optimizers[conf.optimizer](model.parameters(), lr=conf.learning_rate, weight_decay=conf.weight_decay)\n",
        "    scheduler = schedulers[conf.schedule](\n",
        "        optimizer, conf.learning_rate,\n",
        "        epochs=conf.epochs,\n",
        "        steps_per_epoch=math.ceil(num_examples / batch_size),\n",
        "    )\n",
        "\n",
        "    loss_func = loss_functions[conf.loss_func]\n",
        "    it = tqdm(range(1, conf.epochs+1))\n",
        "    \n",
        "    best_loss = float(\"inf\")\n",
        "    step = 0\n",
        "\n",
        "    for epoch in it:\n",
        "\n",
        "        #Each epoch has a training and validation phase\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        for phase in ['train','val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "                batches = train_loader\n",
        "            else:\n",
        "                model.eval()  # Set model to evaluate mode\n",
        "                batches = val_loader\n",
        "            for i, tensors in enumerate(batches):\n",
        "                cats, inputs, forecasts, actuals, *loss_args = map(lambda x: x.to(device), tensors)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                y_pred = model(cats, inputs.float()).unsqueeze(2)\n",
        "\n",
        "                prediction = torch.matmul(forecasts, y_pred).squeeze(2)\n",
        "                loss = loss_func(prediction, actuals, *loss_args)\n",
        "                if phase == 'train':\n",
        "                    train_losses.append(loss.item())\n",
        "                    loss.backward()\n",
        "\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    \n",
        "                    step += 1\n",
        "                else:\n",
        "                    val_losses.append(loss.item())\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        val_loss = np.mean(val_losses)\n",
        "\n",
        "        if val_loss < best_loss: \n",
        "            best_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            filepath = \"model.pth\"\n",
        "            torch.save(best_model_wts, filepath)\n",
        "            wb.save(filepath)\n",
        "\n",
        "        wb.log({\"train_loss\": train_loss, \"val_loss\": val_loss, \"epoch\": epoch, \"best_loss\": best_loss, \"n_examples\":batch_size*step, \"lr\": optimizer.param_groups[0][\"lr\"]}, step=step)\n",
        "        it.set_postfix({\"train_loss\": train_loss, \"val_loss\": val_loss, \"best_loss\": best_loss, \"lr\": f'{optimizer.param_groups[0][\"lr\"]:.2e}'})\n",
        "        \n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RXNHySSwjXyY"
      },
      "outputs": [],
      "source": [
        "def train(config=None, project=None, entity=None, enablewb=True):\n",
        "    mode = \"online\" if enablewb else \"online\"\n",
        "    with wb.init(config=config, project=project, entity=entity, job_type=\"training\", mode=mode) as run:\n",
        "        conf = run.config\n",
        "        print(conf)\n",
        "\n",
        "        datapath = artifact_to_path(run, conf.data)\n",
        "        splitpath = artifact_to_path(run, conf.data_split)\n",
        "\n",
        "        (\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            emb_dims,\n",
        "            num_cont,\n",
        "            num_examples,\n",
        "        ) = ensemble_loaders(\n",
        "                    datapath=datapath, splitpath=splitpath,\n",
        "                    batch_size=conf.batch_size,\n",
        "                    feature_set=conf.feature_set,\n",
        "                    n_models=conf.num_models,)\n",
        "        \n",
        "        model = weightnets[conf.architecture](\n",
        "            num_cont=num_cont,\n",
        "            out_size=conf.num_models,\n",
        "            n_hidden=conf.n_hidden,\n",
        "            hidden_dim=conf.hidden_dim,\n",
        "            dropout=conf.dropout,\n",
        "            bn=conf.bn,\n",
        "            activation=conf.act,\n",
        "            emb_dims=emb_dims,\n",
        "        )\n",
        "\n",
        "        print(f\"Moving model to device: {device}\")\n",
        "        model = model.float().to(device)\n",
        "\n",
        "        model = train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            num_examples,\n",
        "            conf=conf,\n",
        "        )\n",
        "    return model\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7-FhB-RV27-"
      },
      "source": [
        "## Train with k-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pVWsTIPWdMPa"
      },
      "outputs": [],
      "source": [
        "def train_model_kfold(model, train_loader, val_loader, num_examples, conf, fold_num):    \n",
        "    batch_size = conf.batch_size\n",
        "\n",
        "    optimizer = optimizers[conf.optimizer](model.parameters(), lr=conf.learning_rate, weight_decay=conf.weight_decay)\n",
        "    scheduler = schedulers[conf.schedule](\n",
        "        optimizer, conf.learning_rate,\n",
        "        epochs=conf.epochs,\n",
        "        steps_per_epoch=math.ceil(num_examples / batch_size),\n",
        "    )\n",
        "\n",
        "    loss_func = loss_functions[conf.loss_func]\n",
        "    it = tqdm(range(1, conf.epochs+1), desc=f\"Fold {fold_num+1} of {conf.k}\")\n",
        "    \n",
        "    for epoch in it:\n",
        "\n",
        "        #Each epoch has a training and validation phase\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        for phase in ['train','val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "                batches = train_loader\n",
        "            else:\n",
        "                model.eval()  # Set model to evaluate mode\n",
        "                batches = val_loader\n",
        "            for i, tensors in enumerate(batches):\n",
        "                cats, inputs, forecasts, actuals, *loss_args = map(lambda x: x.to(device), tensors)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                y_pred = model(cats, inputs.float()).unsqueeze(2)\n",
        "\n",
        "                prediction = torch.matmul(forecasts, y_pred).squeeze(2)\n",
        "                loss = loss_func(prediction, actuals, *loss_args)\n",
        "                if phase == 'train':\n",
        "                    train_losses.append(loss.item())\n",
        "                    loss.backward()\n",
        "\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                else:\n",
        "                    val_losses.append(loss.item())\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        val_loss = np.mean(val_losses)\n",
        "\n",
        "        wb.log({f\"train_loss/{fold_num}\": train_loss, f\"val_loss/{fold_num}\": val_loss, \"epoch\": epoch, \"lr\": optimizer.param_groups[0]['lr']})\n",
        "        it.set_postfix({\"train_loss\": train_loss, \"val_loss\": val_loss, \"lr\": f\"{optimizer.param_groups[0]['lr']:.2e}\"})\n",
        "\n",
        "    return val_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize(df, scaler=None):\n",
        "    feats = df.loc[:, \"x_acf1\":\"lstm_31\"]\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler().fit(feats)\n",
        "\n",
        "    index, columns = feats.index, feats.columns\n",
        "    df.loc[:, \"x_acf1\":\"lstm_31\"] = pd.DataFrame(scaler.transform(feats), index=index, columns=columns)\n",
        "\n",
        "    return df, scaler"
      ],
      "metadata": {
        "id": "2f5mYe86yWLZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "kciUFMSKTqBH"
      },
      "outputs": [],
      "source": [
        "def train_kfold(config=None, project=None, entity=None, enablewb=True):\n",
        "    mode = \"online\" if enablewb else \"online\"\n",
        "    with wb.init(config=config, project=project, entity=entity, job_type=\"training\", mode=mode) as run:\n",
        "        conf = run.config\n",
        "        print(conf)\n",
        "\n",
        "        rnd_seed = np.random.randint(1e9)\n",
        "        run.log({\"random_seed\": rnd_seed})\n",
        "        datapath = artifact_to_path(run, conf.data)\n",
        "\n",
        "        df = pd.read_feather(datapath).set_index(\"m4id\")\n",
        "        \n",
        "        outer_losses = []\n",
        "        for s, seed in enumerate([69, 420, 666]):\n",
        "            df = shuffle(df, random_state=seed)\n",
        "            folds = np.array_split(df, conf.k)\n",
        "            losses = []\n",
        "            for i, val in enumerate(folds, start=(s*conf.k)):\n",
        "                data = pd.concat(folds[:i] + folds[(i+1):], axis=0)\n",
        "                data, scaler = standardize(data, scaler=None)\n",
        "                val, _ = standardize(val, scaler=scaler)\n",
        "\n",
        "                (\n",
        "                    train_loader,\n",
        "                    val_loader,\n",
        "                    emb_dims,\n",
        "                    num_cont,\n",
        "                    num_examples,\n",
        "                ) = ensemble_loaders_kfold(\n",
        "                            data=data, val=val,\n",
        "                            batch_size=conf.batch_size,\n",
        "                            feature_set=conf.feature_set,\n",
        "                            n_models=conf.num_models,\n",
        "                            cpus=None,)\n",
        "\n",
        "                torch.manual_seed(rnd_seed)\n",
        "                model = weightnets[conf.architecture](\n",
        "                    num_cont=num_cont,\n",
        "                    out_size=conf.num_models,\n",
        "                    n_hidden=conf.n_hidden,\n",
        "                    hidden_dim=conf.hidden_dim,\n",
        "                    dropout=conf.dropout,\n",
        "                    bn=conf.bn,\n",
        "                    activation=conf.act,\n",
        "                    emb_dims=emb_dims,\n",
        "                )\n",
        "\n",
        "                print(f\"Moving model to device: {device}\")\n",
        "                model = model.float().to(device)\n",
        "\n",
        "                loss = train_model_kfold(\n",
        "                    model,\n",
        "                    train_loader,\n",
        "                    val_loader,\n",
        "                    num_examples,\n",
        "                    conf=conf,\n",
        "                    fold_num=i,\n",
        "                )\n",
        "\n",
        "                losses.append(loss)\n",
        "\n",
        "        overall_loss = np.mean(losses)\n",
        "        run.log({\"overall_loss\": overall_loss})\n",
        "\n",
        "    return overall_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txgaemzlh9DO"
      },
      "source": [
        "## Run config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normal config"
      ],
      "metadata": {
        "id": "i4OdXZWW6hzs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "MyJs3OK-iAxs"
      },
      "outputs": [],
      "source": [
        "norm_config = dict(\n",
        "    k=None,\n",
        "    epochs=20,\n",
        "    hidden_dim=256,\n",
        "    n_hidden=2,\n",
        "    learning_rate=2e-3,\n",
        "    optimizer=\"adamw\",\n",
        "    architecture=\"WeightNetV4\",\n",
        "    data=\"ensemble_traval:standard\",\n",
        "    data_split=\"traval_split_80_20:v0\",\n",
        "    batch_size=1024,\n",
        "    loss_func=\"owa\",\n",
        "    dropout=0.5,\n",
        "    weight_decay=0.05,\n",
        "    bn=False,\n",
        "    feature_set=\"ma\",\n",
        "    act=\"leaky\",\n",
        "    num_models=14,\n",
        "    schedule=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-fold config"
      ],
      "metadata": {
        "id": "F3uSVjLO6c20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kfold_config = dict(\n",
        "    k=5,\n",
        "    epochs=5,\n",
        "    hidden_dim=256,\n",
        "    n_hidden=2,\n",
        "    learning_rate=2e-3,\n",
        "    optimizer=\"adamw\",\n",
        "    architecture=\"WeightNetV4\",\n",
        "    data=\"ensemble_traval:non-standard\",\n",
        "    batch_size=1024,\n",
        "    loss_func=\"owa\",\n",
        "    dropout=0.5,\n",
        "    weight_decay=0.05,\n",
        "    bn=False,\n",
        "    feature_set=\"ma\",\n",
        "    act=\"leaky\",\n",
        "    num_models=14,\n",
        "    schedule=None,\n",
        ")"
      ],
      "metadata": {
        "id": "w7_Pzse86cOO"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnxXZCSOiCnW"
      },
      "source": [
        "## Start run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        },
        "id": "wE0qbeQFwyk0",
        "outputId": "6503456a-c9d1-442d-a380-04da6e1a0444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2h2oz37b with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4495462534810624\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0022238325304523507\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_hidden: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.032496945846470844\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/krankile/weight-net/runs/2h2oz37b\" target=\"_blank\">effortless-sweep-1</a></strong> to <a href=\"https://wandb.ai/krankile/weight-net\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/krankile/weight-net/sweeps/4r44tbf3\" target=\"_blank\">https://wandb.ai/krankile/weight-net/sweeps/4r44tbf3</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 128, 'dropout': 0.4495462534810624, 'epochs': 11, 'hidden_dim': 64, 'learning_rate': 0.0022238325304523507, 'n_hidden': 2, 'weight_decay': 0.032496945846470844, 'k': 5, 'optimizer': 'adamw', 'architecture': 'WeightNetV4', 'data': 'ensemble_traval:non-standard', 'data_split': 'traval_split_80_20:v0', 'loss_func': 'owa', 'bn': False, 'feature_set': 'ma', 'act': 'leaky', 'num_models': 14, 'schedule': None}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ensemble_traval:non-standard, 119.61MB. 1 files... Done. 0:0:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU count: 2\n",
            "Loaded df of shape (79996, 801)\n",
            "Loaded df of shape (19999, 801)\n",
            "Moving model to device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 1 of 5: 100%|██████████| 11/11 [01:30<00:00,  8.25s/it, train_loss=0.767, val_loss=0.775, lr=2.22e-03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU count: 2\n",
            "Loaded df of shape (79996, 801)\n",
            "Loaded df of shape (19999, 801)\n",
            "Moving model to device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 2 of 5: 100%|██████████| 11/11 [01:31<00:00,  8.36s/it, train_loss=0.771, val_loss=0.777, lr=2.22e-03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU count: 2\n",
            "Loaded df of shape (79996, 801)\n",
            "Loaded df of shape (19999, 801)\n",
            "Moving model to device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 of 5: 100%|██████████| 11/11 [01:29<00:00,  8.15s/it, train_loss=0.773, val_loss=0.776, lr=2.22e-03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU count: 2\n",
            "Loaded df of shape (79996, 801)\n",
            "Loaded df of shape (19999, 801)\n",
            "Moving model to device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 4 of 5: 100%|██████████| 11/11 [01:32<00:00,  8.44s/it, train_loss=0.773, val_loss=0.778, lr=2.22e-03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU count: 2\n",
            "Loaded df of shape (79996, 801)\n",
            "Loaded df of shape (19999, 801)\n",
            "Moving model to device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 5 of 5: 100%|██████████| 11/11 [01:32<00:00,  8.39s/it, train_loss=0.77, val_loss=0.824, lr=2.22e-03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU count: 2\n",
            "Loaded df of shape (99995, 801)\n",
            "Loaded df of shape (19999, 801)\n",
            "Moving model to device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 6 of 5: 100%|██████████| 11/11 [01:51<00:00, 10.14s/it, train_loss=0.767, val_loss=0.759, lr=2.22e-03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU count: 2\n",
            "Loaded df of shape (99995, 801)\n",
            "Loaded df of shape (19999, 801)\n",
            "Moving model to device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 7 of 5:  73%|███████▎  | 8/11 [01:20<00:30, 10.13s/it, train_loss=0.774, val_loss=0.772, lr=2.22e-03]"
          ]
        }
      ],
      "source": [
        "sweepid = \"krankile/weight-net/4r44tbf3\"\n",
        "enablewb = True\n",
        "project = \"weight-net\"\n",
        "usecv = True\n",
        "\n",
        "train_func, config = (train_kfold, kfold_config) if usecv else (train, norm_config)\n",
        "\n",
        "if sweepid:\n",
        "    count = 500 # number of runs to execute\n",
        "    wb.agent(sweepid, function=partial(train_func, config=config), count=count)\n",
        "else:\n",
        "    res = train_func(config=config, project=project, entity=\"krankile\", enablewb=enablewb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZHSDEPVJ0Wno"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KMfvZQ5aMnPu",
        "OJJhNFTgq2YF"
      ],
      "name": "2_fit_weight_net.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}